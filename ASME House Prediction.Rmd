---
title: "Regression Analysis of Asme House Prediction"
author: "Muhammad Shahzaib"
date: "3/31/2020"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

loading Libraries

```{r}
library(readr)
library(zoo)
library(tidyverse)
library(reshape2)
```

##   Part1: diving deep in the Data   ##

Initially Selected 40 Variables from The dataset by going through the Variable Description

```{r}
train <- read_csv("train.csv")
columns <- read_csv("columns.txt", col_names = FALSE)

train_sub <- train[ , which(names(train) %in% columns$X1)]
```

After observing the Sumary of Read Data, Corrected Data type of columns 

```{r}
summary(train_sub)

train_sub$OverallQual <- as.factor(train_sub$OverallQual)
train_sub$OverallCond <- as.factor(train_sub$OverallCond)
```

Iterate over DataFrame Columns to change Charater type data as Factors
and droping columns having more than 40% NA values in Factor-Columns.
and sum of NA and Zeros in numeric-Columns 
dropped Columns are: 
[MasVnrArea, 2ndFlrSF, LowQualFinSF, Fireplaces, FireplaceQu, WoodDeckSF, OpenPorchSF, EnclosedPorch,  3SsnPorch, ScreenPorch, PoolArea, PoolQC, FenceMiscVal ]


```{r}
drop = list()
for(i in columns$X1){
  if(class(train_sub[[i]]) == "character"){
    train_sub[i] = as.factor(train_sub[[i]])
    if(sum(is.na(train_sub[[i]])) > dim(train_sub)[1] * 0.4){
      drop = c(drop, i)
    }
  }
  else{
    if((sum(is.na(train_sub[i])) + sum(train_sub[i] == 0, na.rm=TRUE)) > dim(train_sub)[1] * 0.4){
      drop = c(drop, i)
    }
  }
}
train_sub <- train_sub[, !(names(train_sub) %in% drop)]
```

Droping BsmtCond, ExterCond GarageCond and GarageQual because 90% of samples are TA
and there are other Attributes for Explaning Basement and Garage

```{r}
train_sub <- train_sub[, !(names(train_sub) %in% c("BsmtCond", "ExterCond", "GarageQual", "GarageCond"))]
```

##   Part2: Working on Selected Features   ##

Now, We are left with 4 types of Variables:
1 -> Rating like (good, very good, Excellent.. etc )  
2 -> Dates, years and month
3 -> Area, continous Variable
4 -> Different Category

Merged Factor's Level With Lower Count to the Next Level
```{r}
train_sub$LotShape[train_sub$LotShape == "IR3"] <- "IR2"

train_sub$HouseStyle[train_sub$HouseStyle %in% c("SFoyer", "SLvl")] <- "SLvl"
train_sub$HouseStyle[train_sub$HouseStyle %in% c("1.5Fin", "1.5Unf")] <- "1.5Fin"
train_sub$HouseStyle[train_sub$HouseStyle %in% c("2.5Fin", "2.5Unf")] <- "2.5Fin"
```

Also order then as 1, 2, 3 along with combining them

```{r}
train_sub$OverallQual[train_sub$OverallQual %in% c("4", "3", "2")] <- "1"
train_sub$OverallQual[train_sub$OverallQual  %in% c("7", "8")] <- "4"
train_sub$OverallQual[train_sub$OverallQual  %in% c("5")] <- "2"
train_sub$OverallQual[train_sub$OverallQual  %in% c("6")] <- "3"
train_sub$OverallQual[train_sub$OverallQual %in% c("10", "9")] <- "5"

train_sub$OverallCond[train_sub$OverallCond %in% c("2", "3", "4")] <- "1"
train_sub$OverallCond[train_sub$OverallCond %in% c("5")] <- "2"
train_sub$OverallCond[train_sub$OverallCond %in% c("6")] <- "3"
train_sub$OverallCond[train_sub$OverallCond %in% c("7")] <- "4"
train_sub$OverallCond[train_sub$OverallCond %in% c("10", "9", "8")] <- "5"
```

Droping unsed Levels

```{r}
train_sub = droplevels.data.frame(train_sub)
```

Feature Extraction from Years and month
calculated Age from Years provided

```{r}
train_sub$Age <- 2020 - train_sub$YearBuilt
train_sub$GarageAge <- 2020 - train_sub$GarageYrBlt
train_sub$Sold <- ((2020- train_sub$YrSold)*12 + train_sub$MoSold)
train_sub$LastRemodAdd <- 2020 - train_sub$YearRemodAdd 


train_sub <- train_sub[, !(names(train_sub) %in% c("YearBuilt", "GarageYrBlt", "MoSold", "YrSold", "YearRemodAdd"))] 

```

Now Dealing NA values,
Area putting Zero because NA means no Garage
LotFortange is interpolated with mean
GarageAge is interpolated with -1
and TA in Heating QC
BsmtExposure with No
GarageFinish with No

```{r}
train_sub$LotFrontage[is.na(train_sub$LotFrontage)] <- mean(train_sub$LotFrontage, na.rm = TRUE)
train_sub$GarageAge[is.na(train_sub$GarageAge)] <- -1
train_sub$GarageArea[is.na(train_sub$GarageArea)] <- 0
train_sub$BsmtExposure[is.na(train_sub$BsmtExposure)] <- "No"
train_sub$HeatingQC[is.na(train_sub$HeatingQC)] <- "TA"

NewLevels <- c(levels(train_sub$GarageFinish), "No")
train_sub$GarageFinish  <- factor(train_sub$GarageFinish, NewLevels)
train_sub$GarageFinish[is.na(train_sub$GarageFinish)] <- "No"
```

Now Changing Factor to Numeric Representative

```{r}
train_subN <- train_sub
for(i in names(train_subN)){
    if(class(train_sub[[i]]) == "factor"){
      train_subN[i] = as.numeric(train_subN[[i]])
    }
}
```

calculated and visualized Corelation among SalePrice and other variables

```{r}
corelations <- cor(train_subN)
melted_cormat <- melt(corelations)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  theme_minimal() + 
  scale_fill_gradient2(low = "blue", high = "red", mid = "black", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Select Attributes with abs(corelation) with SalesPrice > 0.5 

```{r}
VarSelected <- c("OverallQual", "OverallCond", "TotalBsmtSF", "1stFlrSF", "GarageArea", 
                "Age", "LastRemodAdd", "ExterQual", "HeatingQC", "GarageFinish", "SalePrice")
train_subN <- train_subN[, (names(train_subN) %in% VarSelected)]
```

Observed Corelation Among Variables and eleminated variables with asb(corelation) > 0.5 
and also eliminated OvaerAllCond as it is not corelated to Sales Prices

```{r}
corelations2 <- cor(train_subN)
melted_cormat2 <- melt(corelations2)
ggplot(data = melted_cormat2, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  theme_minimal() + 
  scale_fill_gradient2(low = "blue", high = "red", mid = "WHITE", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

VarDrop <- c("TotalBsmtSF", "ExterQual", "LastRemodAdd", "GarageFinish", "OverallCond")
train_subN <- train_subN[, !(names(train_subN) %in% VarDrop)]
```

Visualizing the relation Of variables
  a)Box Plot of the variables:
  
```{r}
boxplot(train_subN$`1stFlrSF`)
boxplot(train_subN$GarageArea)
boxplot(train_subN$Age)
boxplot(train_subN$SalePrice)
```
  
  b)Scatter Plot

```{r}
ggplot(train_subN, aes(x=SalePrice, y=GarageArea)) + 
  geom_point(aes(color=Age))

ggplot(train_subN, aes(x=SalePrice, y=`1stFlrSF`)) + 
  geom_point(aes(color=Age))

ggplot(train_subN, aes(x=`1stFlrSF`, y=GarageArea)) + 
  geom_point(aes(color=Age))

ggplot(train_subN, aes(x=OverallQual, y=SalePrice)) + 
  geom_point(aes(color=Age))

ggplot(train_subN, aes(x=HeatingQC, y=SalePrice)) + 
  geom_point(aes(color=`1stFlrSF`))
```

##   Part3: Summarized implementation of the above justyfied Decisions   ##

Writing Few Functions for Reusibiity:

1) outlierDetector:

```{r}
outlier_Detector <- function(data){
  outliers <- list()
  for(i in c("1stFlrSF", "GarageArea", "Age", "SalePrice")){
    outliers[[i]] <-  boxplot(data[[i]], plot = F)$stats
  }
  return(outliers)
}
```

2) Outlier Remover, providing the limilts(boxplot()$stats):

```{r}
remove_outlier <- function(data, outliers){
  for(i in names(outliers)){
    data[[i]][data[i] > outliers[[i]][5]] <- as.numeric(outliers[[i]][4])
    data[[i]][data[i] < outliers[[i]][1]] <- as.numeric(outliers[[i]][2])
  }
  return(data)
}
```

3) Function to Read Training data from csv(only selected Features)
  and then combining categories as we did in Data cleaning
  and then Dealing the outliers by using Function 1 and 2:
  
```{r}
read_train <- function(name){
  data <- read_csv(name)
  columns <- c("HeatingQC", "OverallQual", "1stFlrSF", "GarageArea", 
               "SalePrice", "YearBuilt")
  data <- data[ , which(names(data) %in% columns)]
  
  data$OverallQual <- as.factor(data$OverallQual)
  data$HeatingQC <- as.factor(data$HeatingQC)
  
  data$OverallQual[data$OverallQual %in% c("4", "3", "2")] <- "1"
  data$OverallQual[data$OverallQual  %in% c("7", "8")] <- "4"
  data$OverallQual[data$OverallQual  %in% c("5")] <- "2"
  data$OverallQual[data$OverallQual  %in% c("6")] <- "3"
  data$OverallQual[data$OverallQual %in% c("10", "9")] <- "5"
  
  data$HeatingQC[data$HeatingQC %in% c("Po", "Fa")] <- "TA"
  
  data$Age <- 2020 - data$YearBuilt
  data <- data[, which(!names(data) %in% c("YearBuilt"))]
  
  data$OverallQual <- as.numeric(data$OverallQual)
  data$HeatingQC <- as.numeric(data$HeatingQC)
  
  outliers <- outlier_Detector(data)
  data <- remove_outlier(data, outliers)
  return (data)
}
```
  
4) Function to Read Testing Data, with no outlier removal

```{r}
read_test <- function(name){
  data <- read_csv(name)
  columns <- c("HeatingQC", "OverallQual", "1stFlrSF", "GarageArea", 
               "YearBuilt")
  data <- data[ , which(names(data) %in% columns)]
  
  data$OverallQual <- as.factor(data$OverallQual)
  data$HeatingQC <- as.factor(data$HeatingQC)
  
  data$OverallQual[data$OverallQual %in% c("4", "3", "2")] <- "1"
  data$OverallQual[data$OverallQual  %in% c("7", "8")] <- "4"
  data$OverallQual[data$OverallQual  %in% c("5")] <- "2"
  data$OverallQual[data$OverallQual  %in% c("6")] <- "3"
  data$OverallQual[data$OverallQual %in% c("10", "9")] <- "5"
  
  data$HeatingQC[data$HeatingQC %in% c("Po", "Fa")] <- "TA"
  
  data$OverallQual <- as.numeric(data$OverallQual)
  data$HeatingQC <- as.numeric(data$HeatingQC)
  
  data$HeatingQC[is.na(data$HeatingQC)] <- median(data$HeatingQC, na.rm=TRUE)
  data$OverallQual[is.na(data$OverallQual)] <- median(data$OverallQual, na.rm=TRUE)
  data$`1stFlrSF`[is.na(data$`1stFlrSF`)] <- mean(data$`1stFlrSF`, na.rm=TRUE)
  data$GarageArea[is.na(data$GarageArea)] <- 0
  data$YearBuilt[is.na(data$YearBuilt)] <- median(data$YearBuilt, na.rm=TRUE)
  
  data$Age <- 2020 - data$YearBuilt
  data <- data[, which(!names(data) %in% c("YearBuilt"))]
  
  
  return (data)
}
```

##   Part4: Final Study of selected variabls and Visualizations   ##

Read Training Data using function and Plot Box-Wishker plot

```{r}
Train_data <- read_train("train.csv")

boxplot(Train_data$`1stFlrSF`)
boxplot(Train_data$GarageArea)
boxplot(Train_data$Age)
boxplot(Train_data$SalePrice)

```

Visualizing the relation between Selected Attributes

```{r}
ggplot(Train_data, aes(x=SalePrice, y=GarageArea)) + 
  geom_point(aes(color=Age))

ggplot(Train_data, aes(x=SalePrice, y=`1stFlrSF`)) + 
  geom_point(aes(color=Age))

ggplot(Train_data, aes(x=`1stFlrSF`, y=GarageArea)) + 
  geom_point(aes(color=Age))

ggplot(Train_data, aes(x=OverallQual, y=SalePrice)) + 
  geom_point(aes(color=Age))

ggplot(Train_data, aes(x=HeatingQC, y=SalePrice)) + 
  geom_point(aes(color=`1stFlrSF`))

```

##   Part5: Regression  Analysis   ##

Function to Model Linear Regressor
and Plot Graph

```{r}
Regression <- function(x, y, var){
  relation <- lm(y~x)
  plot(y,x,col = "blue",main = paste("Price Vs", var, sep=" "), 
       abline(lm(x~y)),ylab = "Price")
}
```

Plot and Vizualize the model Line as Price as y and each Attribute at X separatly

```{r}
Regression(Train_data$SalePrice, Train_data$Age, "Age")
Regression(Train_data$SalePrice, Train_data$`1stFlrSF`, "1stFlrSF")
Regression(Train_data$SalePrice, Train_data$GarageArea, "GarageArea")
Regression(Train_data$SalePrice, Train_data$HeatingQC, "HeatingQC")
Regression(Train_data$SalePrice, Train_data$OverallQual, "OverallQual")
```

Now Modeling the Multi-Linear Model using all the selected variables

```{r}
model <- lm(SalePrice~OverallQual+HeatingQC+`1stFlrSF`+GarageArea+Age, data = Train_data)

print(model)
```

##   Part6: Working on testing Data   ##

Importing Data from CSV

```{r}
test_data <- read_test("test.csv")
```

Wrote Function to Predicte y_hat using model coef

```{r}
predict <- function(model, X){
  y <- as.numeric(coef(model)[1])
  for(i in 2:length(coef(model))){
    y <- y + as.numeric((coef(model)[i]*X[i-1]))
  }
  return(y)
}
```

predicting y_hat using multi linear model

```{r}
y_hat <- list()
for (i in 1:nrow(test_data)){
  y_hat[i] <- predict(model, as.numeric(test_data[i,]))
}
test_data$SalePrice <- as.numeric(y_hat)
```

visualizing the training-> salePrice(Blue)
and Predicted -> SalePrice(red)


```{r}
ggplot() + 
  geom_line(data= Train_data, aes(x = Age, y = SalePrice), color = "blue") + 
  geom_line(data= test_data, aes(x = Age, y = SalePrice), color = "red")

ggplot() + 
  geom_line(data= Train_data, aes(x =`1stFlrSF`, y = SalePrice), color = "blue") + 
  geom_line(data= test_data, aes(x = `1stFlrSF`, y = SalePrice), color = "red")

ggplot() + 
  geom_line(data= Train_data, aes(x = GarageArea, y = SalePrice), color = "blue") + 
  geom_line(data= test_data, aes(x = GarageArea, y = SalePrice), color = "red")

ggplot() + 
  geom_line(data= Train_data, aes(x = HeatingQC, y = SalePrice), color = "blue") + 
  geom_line(data= test_data, aes(x = HeatingQC, y = SalePrice), color = "red")

ggplot() + 
  geom_line(data= Train_data, aes(x = OverallQual, y = SalePrice), color = "blue") + 
  geom_line(data= test_data, aes(x = OverallQual, y = SalePrice), color = "red")

```

## Conclusion ##
1) There Were 80 Variables from which I focused on Continous and Ordinal Attributes.
  I went Throught he description of each Attribute.
2) after Selecting 40 attribute i elemenated Features where more than 50% of the time they  belong to specific category and there were other Feature describing the same thing.
3) At That stage I was down to 20 Variables.
4) based of their Inter-Corelation and Corelation with the House Price, I elemenated
  features where there were less than 0.5 abs(corelation)
5) Till that stage, I was left with 5 variables.
6) Detected Outliers and interpolated NA values.
7) Produces Lm of individual Attribute with SalePrice
8) Ploted Scatter plot of Attribute VS SalePrice
9) Produced MultiLinear Model
10) Predicted House Price of Test Data
11) Compared the given House Prices Vs given Attribute
    and  Predicted House Price VS the Given test Attributes
